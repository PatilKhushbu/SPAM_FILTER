# ğŸ“§ Spam Filter with Support Vector Machine and Naive Bayes

This project implements and compares discriminative and generative classifiers to build a **spam filter**, using the [Spambase dataset](https://archive.ics.uci.edu/ml/datasets/spambase). The classifiers are:

- ğŸ§  **Support Vector Machine (SVM)** with various kernels (Linear, Polynomial, RBF, and Angular)
- ğŸ“Š **Naive Bayes** assuming multivariate Gaussian distributions

## ğŸ—‚ï¸ Project Structure

```
ğŸ“ spam-filter-classifier/
â”œâ”€â”€ data/              # Preprocessed dataset (TF-IDF transformed)
â”œâ”€â”€ models/            # Trained models
â”œâ”€â”€ notebooks/         # Jupyter notebooks for experimentation
â”œâ”€â”€ src/               # Source code: training, evaluation, utils
â”œâ”€â”€ results/           # Cross-validation results and plots
â””â”€â”€ README.md
```

## ğŸ“Œ Objective

> Build a binary spam filter using machine learning, compare the effectiveness of discriminative and generative models, and explore the impact of vector normalization (angular information).

---

## ğŸ§ª Dataset

The **Spambase** dataset represents email messages with:
- 48 word frequencies
- 6 character frequencies
- 1 binary label (spam = 1, ham = 0)

After removing irrelevant features, we apply **TF-IDF** transformation before classification.

---

## ğŸ” Classification Models

### âœ… Support Vector Machine (SVM)

- **Kernels Used**:
  - Linear
  - Polynomial (Degree = 2)
  - Radial Basis Function (RBF)
  - Cosine similarity (Angular kernel)

- **Highlights**:
  - Performance improves significantly with vector normalization
  - Achieves up to **97.6% accuracy** with cosine kernel

### ğŸ“ˆ Naive Bayes

- Assumes **conditional independence** and **multivariate normality**
- Fast training time
- Less accurate than SVM (~80% mean accuracy)

- **Assumption Validation**:
  - âŒ Mardia and Henze-Zirkler tests reject Gaussianity
  - âŒ Chi-squared tests reject feature independence

---

## âš™ï¸ Tools & Libraries

- Python 3.x
- Scikit-learn (`SVM`, `NaiveBayes`, `TF-IDF`, `Cross-validation`)
- NumPy, Pandas, Matplotlib
- R (for statistical normality tests)

---

## ğŸ“Š Results

| Kernel Type           | Mean Accuracy | Min Accuracy | Max Accuracy |
|-----------------------|---------------|---------------|---------------|
| Linear                | 60.81%        | 60.52%        | 61.30%        |
| Polynomial (d=2)      | 60.59%        | 60.52%        | 60.65%        |
| RBF                   | 60.59%        | 60.52%        | 60.65%        |
| **Cosine (Angular)**  | **97.63%**    | 90.19%        | 93.92%        |
| **Naive Bayes**       | **80.51%**    | 70.21%        | 85.86%        |

> ğŸ” Angular kernels outperform traditional ones by removing vector length dependency.

---

## ğŸ§  Key Insights

- SVM with angular kernels (cosine similarity) is most effective for text-based features.
- Naive Bayes is computationally efficient but suffers if data assumptions are violated.
- Normalizing vectors before classification improves robustness and accuracy.

---

## ğŸ“Œ Authors

- **Khushbu Mahendra Patil** â€” Matric. No: 882697  
- **Khalid Vafa** â€” Matric. No: 882677  
- ğŸ“˜ Course: Artificial Intelligence â€“ Knowledge Representation and Planning  
- ğŸ‘¨â€ğŸ« Submitted to: Prof. Andrea Torsello

---
